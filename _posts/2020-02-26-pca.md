---
layout: post
title: 主成分分析 PCA
categories: [机器学习]
description: 介绍PCA算法
keywords: 机器学习
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

### 2.12 Principal Components Analysis

One simple machine learning algorithm, **principal components analysis(PCA)**, can be derived using only knowledage of basic linear algebra.

Suppose we have a collection of m points $\{x^{(1)},...,x^{(m)}\}$ in $\mathbb R^n$ and we want to apply lossy compression to these points. Lossy compresion means storing the points in a way that requires less memory but may lose some presion. We want to lose as little precision as possible.   
One way we can encode these points is to represent a lower-dimensional version of them. For each point $x^
{(i)}∈\mathbb R^n$ we will find a corresponding code vector $c^{(i)}∈\mathbb R^l$. If $l$ is smaller than $n$,storing the code points will take less memory than storing the original data. We will want to find some encoding function that produces the code for an input, $f(x)=c$, and a decoding function that produces the reconstructed input given its code, $x≈g(f(x))$.

PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back into $\mathbb R^n$. Let $g(c)=Dc$, where $D∈\mathbb R^n×l$ is the matrix defining the decoding.    
To keep the encoding problem easy, PCA constrains the columns of $D$ to be orthogonal to each other. To give the problem a unique solution, we constrain all the columns of $D$ to have unit norm.   
In order to turn this basic idea to an algorithm we can implement, the first thing we need to do is figure out how to generate the optimal code point $c^*$ for each inout point $x$. One way to do this is to minimize the distance between the input point $x$ and its reconstruction, $g(c^*)$. We can measure this distance using a norm. In the principal components algorithm, we use the $L^2$ norm: 
$$c^* = \arg \min_c ||x-g(c)||_2$$
Or its squared form:
$$c^* = \arg \min_c ||x-g(c)||_2^2$$
The function being minimized simplifies to:
$$(x-g(c))^T(x-g(c))$$
$$=x^Tx - x^Tg(c)-g(c)^Tx+g(c)^Tg(c)$$
$$=x^Tx-2x^Tg(c)+g(c)^Tg(c)$$

We can now change the function being minimized again, to omit the first term, since this term does not depend on $c$:
$$c^* = -2x^Tg(c)+g(c)^Tg(c)$$
To make further progress, we must subtitute in the definition of $g(c)$:
$$c^* = \arg\min_c - 2x^TDc + c^TD^TDc$$
$$=\arg\min_c - 2x^TDc + c^TI_lc$$
$$=\arg\min_c - 2x^TDc + c^Tc$$

We can solve this optimization problem by using vector calculus:
$$\nabla_c(-2x^TDc + c^Tc)=0$$
$$-2D^Tx + 2c = 0$$
$$c = D^Tx$$

This makes the algorithm efficient: we can optimally encode $x$ using just a matrix-vector operation. To encode a vector, we apply the encoder function:
$$f(x)=D^Tx$$
Using a further matrix multiplication, we can also define the PCA reconstruction operation:
$$r(x)=g(f(x))=DD^Tx$$
Next, we need to choose the encoding matrix $D$...(未完待续)