---
layout: post
title: 支持向量机(SVM)
categories: 机器学习
description: 机器学习支持向量机方法
keywords: 机器学习, SVM
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

#### 间隔与支持向量

给定训练样本集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}, y_i∈\{-1,+1\}$，分类学习的最基本思想就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开. 但能将训练样本分开的划分超平面可能有很多，如下图所示，我们应该努力去找哪一个呢？

![image-20200220141738767](/images/posts/ml/svm1.png)


直观上看，应该去找位于两类样本“正中间”的划分超平面，即图中粗线的那个，因为该划分超平面对训练样本局部扰动的“容忍”性最好. 换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强.

在样本空间中，划分超平面可通过如下线性方程来描述：

$$w^Tx+b=0$$

其中$w=(w_1;w_2;...;w_d)$为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离. 显然，划分超平面可被法向量w和位移量b确定，下面我们将其记为(w,b). 样本空间中任意点x到超平面(w,b)的距离可写为：

$$r=\frac{|w^Tx+b|}{||w||}$$

假设超平面(w,b)能将训练样本正确分类，即对于$(x_i,y_i)∈D$，若$y_i=+1$，则有$w^Tx_i+b>0$；若$y_i=-1$，则有$w^Tx_i+b<0$. 令

![image-20200220141822669](//images/posts/ml/svm2.png)


如下图所示，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为“支持向量”(support vector)，两个异类支持向量到超平面的距离之和为：

$$\gamma = \frac{2}{||w||}$$

它被称为“间隔”(margin)

![image-20200220142226114](/images/posts/ml/svm3.png)

欲找到具有“最大间隔”(maximum margin)的划分超平面，也就是要找到能满足约束的参数w和b，使得$\gamma$最大，即

$\max_{w,b} \frac{2}{||w||}$

显然，为了最大化间隔，仅需最大化$||w||^{-1}$这等价于最小化$||w||^2$，即

$\min_{w,b}\frac{1}{2}||w||^2$

这就是支持向量机(Support Vector Machine，简称SVM)的基本形式.

#### 核函数

在前面的讨论中，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类. 然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，例如下图的异或问题就不是线性可分的：

![image-20200220142255076](/images/posts/ml/svm4.png)

对于这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分. 例如上图中，若将原始的二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面. 幸运的是，如果原始空间是有限维的，即属性数有限，那么一定存在一个高纬特征空间使样本可分.

令$\phi(x)$表示将x映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为：$f(x)=w^T\phi(x)+b$

其中w和b是模型参数，其对偶问题是：

![image-20200220142348197](/images/posts/ml/svm5.png)


求解上式涉及到计算$\phi(x_i)^T\phi(x_j)$，这是样本$x_i$与$x_j$映射到特征空间之后的内积. 由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算$\phi(x_i)^T\phi(x_j)$是非常困难的. 为了避开这个障碍，可以设想这样一个函数：

$$k(x_i,x_j)=⟨\phi(x_i),\phi(x_j)⟩=\phi(x_i)^T\phi(x_j)$$

即$x_i$与$x_j$在特征空间的内积等于它们在原始空间样本中通过函数$k(.,.)$计算的结果. 有了这个函数，我们就不必直接去计算高维特征空间中的内积，于是上式可重写为：

![image-20200220142414506](/images/posts/ml/svm6.png)


求解后得到：

![image-20200220142438977](/images/posts/ml/svm7.png)


这里的函数$k(.,.)$就是“核函数”(kernel function). 上式显示出模型最优解可通过训练样本的核函数展开，这一展式亦称“支持向量展式”(support vector expansion).

显然，若已知合适映射$\phi(.)$的具体形式，则可以写出核函数$k(.,.)$. 但在现实任务中，我们通常不知道$\phi(.)$是什么形式，那么，合适的核函数是否一定存在呢，什么样的函数能做核函数呢？我们有下面的定理：

核函数：令$X$为输入空间，$k(.,.)$是定义在$X×X$上的对称函数，则$k$是核函数当且仅当对于任意数据$D=\{x_1,x_2,...,x_m\}$，“核矩阵”(kernel matrix)$K$总是半正定的：

![image-20200220142505683](/images/posts/ml/svm8.png)


事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$. 换言之，任何一个核函数都隐式地定义了一个称为“再生核希尔伯特空间”(reproducing Kernel Hilbert Space，简称RKHS)的特征空间.

通过前面的讨论可知，我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要. 需要注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间. 于是，“核函数选择”称为支持向量机的最大变数. 若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳，下标列出了几种常用的核函数：

![image-20200220142525093](/images/posts/ml/svm9.png)

此外，核函数还可通过函数组合得到，例如：

\* 若$k_1$和$k_2$为核函数，则对于任意正数$\gamma_1,\gamma_2$，其线性组合：

$$\gamma_1k_1 + \gamma_2k_2$$

也是核函数

\* 若$k_1$和$k_2$为核函数，则核函数的直积：

$k_1⊗k_2(x, z)=k_1(x,z)k_2(x,z)$

也是核函数

\* 若$k_1$为核函数，则对于任意函数$g(x)$，

$k(x,z)=g(x)k_1(x,z)g(z)$

也是核函数.

#### 核方法

人们发展出一系列基于核函数的学习方法，统称为“核方法”(kernel methods). 最常见的是，通过“核化”来将线性学习器拓展为非线性学习器. 下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性拓展，从而得到“核线性判别分析”(Kernelized Linear Discriminant Analysis，简称KLDA).

我们先假设可通过某种映射$\phi:X\mapsto \mathbb F$将样本映射到一个特征空间$\mathbb F$，然后在$\mathbb F$中执行线性判别分析，以求得：

$h(x)=w^T\phi(x)$

![image-20200220142605208](/images/posts/ml/svm10.png)
![image-20200220142620884](/images/posts/ml/svm11.png)
![image-20200220142637475](/images/posts/ml/svm12.png)

![image-20200220144433100](/images/posts/ml/svm13.png)