---
layout: post
title: 深度前馈网络 part1
categories: [深度学习]
description: 简要介绍深度学习中的深度前馈神经网络
keywords: 深度学习
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**深度前馈网络**（deep feedforward network），也叫**前馈神经网络**（feedforward neural network）或者多层感知机（multilayer perceptron, MLP），是典型的深度学习模型。前馈网络的目标是近似某个函数$f^*$。例如，对于分类器，$y=f^*(x)$将输入$x$映射到一个类别$y$，前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使它能够得到最佳的函数近似。

### 6.1 学习异或（XOR）

我们可以把这个问题当作是回归问题，并且使用均方误差损失函数：
$$J(\theta) = \frac{1}{N}\sum_{x∈X}(f^*(x) - f(x;\theta))^2$$

* $f^*(x):$目标函数
* $f(x;\theta)$: 模型给出的函数

因为异或问题不能使用线性函数来很好地求解，因此，我们需要使用非线性函数来描述这些特征。 **大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目标**
默认的推荐是使用由激活函数$g(z) = max\{0, z\}$定义的**整流线性单元(rectifield linear unit)** 或者称为 **ReLU**.
现在我们可以指明我们的整个网络是：
$$f(x; W,c,w,b) = w^T max\{0, W^Tx+c
\} + b$$

### 6.2 基于梯度的学习

神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常用迭代的，基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是保证全局收敛。凸优化从任何一种初始化参数出发都会收敛。用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。**对于前馈神经网络，将所有的权重初始化为小随机数是很重要的，偏置可以初始化为0或者小的正数。**

#### 6.2.1 代价函数

深度神经网络设计中的一个重要方面是代价函数的选择，在大多数情况下，我们参数模型定义来一个分布$p(y|x;\theta)$并且我们简单地使用最大似然原理。**这意味着我们使用训练数据和模型预测间的交叉熵作为代价函数**。

##### 6.2.1.1 使用最大似然学习条件分布

大多数现代神经网络使用**最大似然**来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价，这个代价函数表示为：
$$J(\theta) = -\mathbb E_{x,y \sim \hat{p}_{data}}\log P_{model}(y|x)$$
代价函数的具体形式随着模型而改变，取决于$\log p_{model}$的具体形式。上述方程通常会有一些不依赖于模型的参数，我们可以舍去。
使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。**明确一个模型p(y|x),则自动地确定了一个代价函数$\log p(y|x)$**.
用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就是当它被应用于实践中常用的模型时，它通常没有最小值。

#### 6.2.2 输出单元

##### 6.2.2.1 用于高斯输出分布的线性单元

一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元。
给定特征$h$，线性输出单元层产生一个向量：$\hat{y}=W^Th + b$
线性输出层经常被用来产生条件高斯分布的均值：
$$p(y|x) = N(y;\hat{y},I)$$
最大化其对数似然此时等价于最小化均方误差。

##### 6.2.2.2 用于Bernoulli输出分布的sigmoid单元

许多任务需要预测二值变量$y$的值，通常我们用sigmoid函数来定义输出单元解决此类问题。

##### 6.2.2.3 用于Multinoulli输出分布的softmax单元

任何时候当我们想要表示一个具有$n$个可能取值的离散型随机变量的分布时，我们都可以使用$softmax$函数。它最常用作分类器的输出，来表示n个不同类上的概率分布。
$$softmax(z)_i = \frac{exp(z_i)}{\sum_jexp(z_j)}$$
where $z_i = \log \hat{P}(y=i | x)$

